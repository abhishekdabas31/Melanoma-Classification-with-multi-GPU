{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import time\n",
    "from colored import fg, attr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.models as model\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor \n",
    "from PIL import Image, ImageFile\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "from torch import optim\n",
    "from torch import FloatTensor, LongTensor, DoubleTensor\n",
    "from albumentations import Normalize, VerticalFlip, HorizontalFlip, Compose\n",
    "from torch.utils.data.sampler import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
      "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
      "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
      "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
      "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
      "\n",
      "  diagnosis benign_malignant  target  \n",
      "0   unknown           benign       0  \n",
      "1   unknown           benign       0  \n",
      "2     nevus           benign       0  \n",
      "3   unknown           benign       0  \n",
      "4   unknown           benign       0  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df = train_df.head(2000)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89d7a1d8f2db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "image = os.path.join(image_path,str(df[\"image_name\"][i])+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folds\n",
    "df = train_df\n",
    "df[\"kfold\"] = -1    \n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.target.values\n",
    "kf = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f\n",
    "\n",
    "\n",
    "# df.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0258065</td>\n",
       "      <td>IP_0454573</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0637539</td>\n",
       "      <td>IP_0583343</td>\n",
       "      <td>male</td>\n",
       "      <td>65.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0219493</td>\n",
       "      <td>IP_7770500</td>\n",
       "      <td>male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0616348</td>\n",
       "      <td>IP_9992027</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0205127</td>\n",
       "      <td>IP_4520332</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>ISIC_0166935</td>\n",
       "      <td>IP_1583003</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>ISIC_0669766</td>\n",
       "      <td>IP_8691359</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>ISIC_0659878</td>\n",
       "      <td>IP_5119720</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>ISIC_0292349</td>\n",
       "      <td>IP_8136122</td>\n",
       "      <td>female</td>\n",
       "      <td>40.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>ISIC_0633126</td>\n",
       "      <td>IP_1242186</td>\n",
       "      <td>male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name  patient_id     sex  age_approx  \\\n",
       "0     ISIC_0258065  IP_0454573    male        55.0   \n",
       "1     ISIC_0637539  IP_0583343    male        65.0   \n",
       "2     ISIC_0219493  IP_7770500    male        60.0   \n",
       "3     ISIC_0616348  IP_9992027  female        50.0   \n",
       "4     ISIC_0205127  IP_4520332    male        35.0   \n",
       "...            ...         ...     ...         ...   \n",
       "1995  ISIC_0166935  IP_1583003    male        45.0   \n",
       "1996  ISIC_0669766  IP_8691359    male        55.0   \n",
       "1997  ISIC_0659878  IP_5119720    male        55.0   \n",
       "1998  ISIC_0292349  IP_8136122  female        40.0   \n",
       "1999  ISIC_0633126  IP_1242186    male        60.0   \n",
       "\n",
       "     anatom_site_general_challenge diagnosis benign_malignant  target  kfold  \n",
       "0                            torso   unknown           benign       0      0  \n",
       "1                            torso   unknown           benign       0      0  \n",
       "2                  upper extremity   unknown           benign       0      0  \n",
       "3                  lower extremity   unknown           benign       0      0  \n",
       "4                  lower extremity   unknown           benign       0      0  \n",
       "...                            ...       ...              ...     ...    ...  \n",
       "1995                         torso   unknown           benign       0      4  \n",
       "1996                         torso     nevus           benign       0      4  \n",
       "1997               lower extremity   unknown           benign       0      4  \n",
       "1998                         torso   unknown           benign       0      4  \n",
       "1999               upper extremity   unknown           benign       0      4  \n",
       "\n",
       "[2000 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"/scratch/dabas.a/data/jpeg\"\n",
    "# for i in range(len(df)):\n",
    "#     image = os.path.join(image_path,str(df[\"image_name\"][i])+'.jpg')\n",
    "# #     print(image.shape)\n",
    "    \n",
    "# #     img = Image.open(image)\n",
    "# #     img = np.array(img)\n",
    "# #     print(\"img shape\",img.shape)\n",
    "#     img = cv2.imread(image,cv2.IMREAD_COLOR)\n",
    "#     print(image)\n",
    "#     print(targets)\n",
    "# #     img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_path, targets, transforms=None):\n",
    "        self.image_path = image_path\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = self.image_path[item]\n",
    "        targets = self.targets[item]\n",
    "        img = Image.open(image)\n",
    "        img = np.array(img)\n",
    "#         print(img.shape)\n",
    "        img = cv2.imread(image,cv2.IMREAD_COLOR)\n",
    "#         print(image)\n",
    "#         print(targets)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            sample = {'image':img}\n",
    "            sample = self.transforms(**sample)\n",
    "            img = sample['image']\n",
    "            #img = self.transforms(img)\n",
    "            #print(img.shape)\n",
    "        else:\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "        return torch.tensor(img, dtype=torch.float), torch.tensor(targets, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "def train_fold(fold):\n",
    "    training_path = \"/scratch/dabas.a/data/jpeg/train_resize\"\n",
    "    \n",
    "\n",
    "    df_train = df[df['kfold'] != fold].reset_index(drop=True)\n",
    "    df_valid = df[df['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    train_images = list(df_train.image_name)\n",
    "    train_images = [os.path.join(training_path,i+'.jpg') for i in train_images]\n",
    "    train_targets = df_train.target.values\n",
    "\n",
    "    valid_images = list(df_valid.image_name)\n",
    "    valid_images = [os.path.join(training_path,i+'.jpg') for i in valid_images]\n",
    "    valid_targets = df_valid.target.values\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.RGBShift(r_shift_limit=40),\n",
    "        A.MultiplicativeNoise(p=1.0),\n",
    "        A.RandomBrightness(0.1),\n",
    "        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    valid_transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.RandomBrightness(0.1),\n",
    "        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = MyDataSet(train_images, train_targets, train_transform)\n",
    "    valid_dataset = MyDataSet(valid_images, valid_targets, valid_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True,sampler=None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False,sampler=None)\n",
    "\n",
    "    return train_loader, valid_loader,valid_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_152(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(Resnet_152, self).__init__()\n",
    "        self.model = model\n",
    "        self.ext = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,images):\n",
    "        out = self.model(images)\n",
    "        out = self.ext(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "epoch=0,loss=0.29804735677316785\n",
      "[tensor([[0.0805],\n",
      "        [0.1117],\n",
      "        [0.0729],\n",
      "        [0.0965],\n",
      "        [0.0994],\n",
      "        [0.0736],\n",
      "        [0.0606],\n",
      "        [0.0805],\n",
      "        [0.0641],\n",
      "        [0.1030],\n",
      "        [0.0912],\n",
      "        [0.1200],\n",
      "        [0.1023],\n",
      "        [0.0658],\n",
      "        [0.1492],\n",
      "        [0.0933],\n",
      "        [0.0737],\n",
      "        [0.0552],\n",
      "        [0.0743],\n",
      "        [0.0873],\n",
      "        [0.0690],\n",
      "        [0.0696],\n",
      "        [0.1119],\n",
      "        [0.0874],\n",
      "        [0.0699],\n",
      "        [0.1275],\n",
      "        [0.0971],\n",
      "        [0.0446],\n",
      "        [0.0874],\n",
      "        [0.0951],\n",
      "        [0.0452],\n",
      "        [0.1077],\n",
      "        [0.1055],\n",
      "        [0.0667],\n",
      "        [0.0372],\n",
      "        [0.0681],\n",
      "        [0.0453],\n",
      "        [0.0695],\n",
      "        [0.0806],\n",
      "        [0.1096],\n",
      "        [0.0838],\n",
      "        [0.0494],\n",
      "        [0.0809],\n",
      "        [0.0763],\n",
      "        [0.0732],\n",
      "        [0.0986],\n",
      "        [0.1019],\n",
      "        [0.0730],\n",
      "        [0.0907],\n",
      "        [0.0413],\n",
      "        [0.0954],\n",
      "        [0.1107],\n",
      "        [0.1043],\n",
      "        [0.0755],\n",
      "        [0.0455],\n",
      "        [0.0880],\n",
      "        [0.0786],\n",
      "        [0.1291],\n",
      "        [0.0670],\n",
      "        [0.1111],\n",
      "        [0.1082],\n",
      "        [0.1020],\n",
      "        [0.1017],\n",
      "        [0.0612],\n",
      "        [0.0741],\n",
      "        [0.0673],\n",
      "        [0.0757],\n",
      "        [0.0988],\n",
      "        [0.1058],\n",
      "        [0.0645],\n",
      "        [0.0761],\n",
      "        [0.0524],\n",
      "        [0.0533],\n",
      "        [0.0688],\n",
      "        [0.0559],\n",
      "        [0.0966],\n",
      "        [0.1070],\n",
      "        [0.0435],\n",
      "        [0.0635],\n",
      "        [0.0863],\n",
      "        [0.0955],\n",
      "        [0.0919],\n",
      "        [0.1273],\n",
      "        [0.0910],\n",
      "        [0.0901],\n",
      "        [0.0610],\n",
      "        [0.0963],\n",
      "        [0.1229],\n",
      "        [0.0537],\n",
      "        [0.1041],\n",
      "        [0.0742],\n",
      "        [0.0779],\n",
      "        [0.0878],\n",
      "        [0.0625],\n",
      "        [0.0828],\n",
      "        [0.0975],\n",
      "        [0.0644],\n",
      "        [0.0995],\n",
      "        [0.1143],\n",
      "        [0.1060]]), tensor([[0.0955],\n",
      "        [0.0770],\n",
      "        [0.1198],\n",
      "        [0.0626],\n",
      "        [0.0888],\n",
      "        [0.0498],\n",
      "        [0.0644],\n",
      "        [0.0402],\n",
      "        [0.0875],\n",
      "        [0.0895],\n",
      "        [0.0592],\n",
      "        [0.0672],\n",
      "        [0.1026],\n",
      "        [0.0719],\n",
      "        [0.0813],\n",
      "        [0.0909],\n",
      "        [0.0620],\n",
      "        [0.1098],\n",
      "        [0.0367],\n",
      "        [0.1058],\n",
      "        [0.0589],\n",
      "        [0.0638],\n",
      "        [0.0414],\n",
      "        [0.0293],\n",
      "        [0.0701],\n",
      "        [0.0916],\n",
      "        [0.0625],\n",
      "        [0.1152],\n",
      "        [0.0877],\n",
      "        [0.1312],\n",
      "        [0.0639],\n",
      "        [0.1172],\n",
      "        [0.0878],\n",
      "        [0.0856],\n",
      "        [0.0959],\n",
      "        [0.0665],\n",
      "        [0.1148],\n",
      "        [0.0707],\n",
      "        [0.0935],\n",
      "        [0.0727],\n",
      "        [0.0990],\n",
      "        [0.0707],\n",
      "        [0.0987],\n",
      "        [0.0985],\n",
      "        [0.0974],\n",
      "        [0.1137],\n",
      "        [0.0845],\n",
      "        [0.1181],\n",
      "        [0.0576],\n",
      "        [0.0975],\n",
      "        [0.0974],\n",
      "        [0.0987],\n",
      "        [0.1212],\n",
      "        [0.0802],\n",
      "        [0.1199],\n",
      "        [0.0976],\n",
      "        [0.0778],\n",
      "        [0.0740],\n",
      "        [0.0644],\n",
      "        [0.0408],\n",
      "        [0.0956],\n",
      "        [0.1163],\n",
      "        [0.0559],\n",
      "        [0.0664],\n",
      "        [0.0569],\n",
      "        [0.0700],\n",
      "        [0.0731],\n",
      "        [0.0604],\n",
      "        [0.1320],\n",
      "        [0.0824],\n",
      "        [0.1218],\n",
      "        [0.0669],\n",
      "        [0.0744],\n",
      "        [0.0739],\n",
      "        [0.0834],\n",
      "        [0.0689],\n",
      "        [0.0998],\n",
      "        [0.1200],\n",
      "        [0.0784],\n",
      "        [0.0725],\n",
      "        [0.0707],\n",
      "        [0.0861],\n",
      "        [0.0686],\n",
      "        [0.0711],\n",
      "        [0.0982],\n",
      "        [0.0308],\n",
      "        [0.0825],\n",
      "        [0.1310],\n",
      "        [0.0898],\n",
      "        [0.0583],\n",
      "        [0.1266],\n",
      "        [0.0568],\n",
      "        [0.1099],\n",
      "        [0.0897],\n",
      "        [0.0509],\n",
      "        [0.0690],\n",
      "        [0.0454],\n",
      "        [0.0318],\n",
      "        [0.0568],\n",
      "        [0.0766]]), tensor([[0.1258],\n",
      "        [0.0660],\n",
      "        [0.0866],\n",
      "        [0.0450],\n",
      "        [0.0613],\n",
      "        [0.1088],\n",
      "        [0.0832],\n",
      "        [0.0574],\n",
      "        [0.0851],\n",
      "        [0.0942],\n",
      "        [0.1125],\n",
      "        [0.0339],\n",
      "        [0.1188],\n",
      "        [0.1041],\n",
      "        [0.0703],\n",
      "        [0.0852],\n",
      "        [0.0877],\n",
      "        [0.0808],\n",
      "        [0.0889],\n",
      "        [0.0813],\n",
      "        [0.0597],\n",
      "        [0.0579],\n",
      "        [0.1044],\n",
      "        [0.0626],\n",
      "        [0.0560],\n",
      "        [0.0591],\n",
      "        [0.0745],\n",
      "        [0.0954],\n",
      "        [0.0628],\n",
      "        [0.0695],\n",
      "        [0.1196],\n",
      "        [0.0733],\n",
      "        [0.0938],\n",
      "        [0.0777],\n",
      "        [0.0708],\n",
      "        [0.1190],\n",
      "        [0.0622],\n",
      "        [0.0658],\n",
      "        [0.1334],\n",
      "        [0.0668],\n",
      "        [0.0564],\n",
      "        [0.0595],\n",
      "        [0.1020],\n",
      "        [0.1243],\n",
      "        [0.0632],\n",
      "        [0.1039],\n",
      "        [0.0713],\n",
      "        [0.0554],\n",
      "        [0.0813],\n",
      "        [0.1149],\n",
      "        [0.0728],\n",
      "        [0.0778],\n",
      "        [0.0643],\n",
      "        [0.1017],\n",
      "        [0.1247],\n",
      "        [0.0612],\n",
      "        [0.0847],\n",
      "        [0.0607],\n",
      "        [0.0698],\n",
      "        [0.0832],\n",
      "        [0.0672],\n",
      "        [0.0692],\n",
      "        [0.0704],\n",
      "        [0.0580],\n",
      "        [0.0505],\n",
      "        [0.1174],\n",
      "        [0.0832],\n",
      "        [0.0874],\n",
      "        [0.1021],\n",
      "        [0.0692],\n",
      "        [0.0451],\n",
      "        [0.0887],\n",
      "        [0.0706],\n",
      "        [0.0790],\n",
      "        [0.0726],\n",
      "        [0.0584],\n",
      "        [0.0551],\n",
      "        [0.0686],\n",
      "        [0.0750],\n",
      "        [0.0717],\n",
      "        [0.0784],\n",
      "        [0.0868],\n",
      "        [0.0602],\n",
      "        [0.0777],\n",
      "        [0.0613],\n",
      "        [0.0723],\n",
      "        [0.0904],\n",
      "        [0.0868],\n",
      "        [0.0771],\n",
      "        [0.1169],\n",
      "        [0.0503],\n",
      "        [0.0413],\n",
      "        [0.1035],\n",
      "        [0.0710],\n",
      "        [0.1262],\n",
      "        [0.0794],\n",
      "        [0.1070],\n",
      "        [0.0632],\n",
      "        [0.1000],\n",
      "        [0.0706]]), tensor([[0.1152],\n",
      "        [0.0974],\n",
      "        [0.1015],\n",
      "        [0.1059],\n",
      "        [0.1206],\n",
      "        [0.0763],\n",
      "        [0.0342],\n",
      "        [0.0428],\n",
      "        [0.0780],\n",
      "        [0.0643],\n",
      "        [0.0718],\n",
      "        [0.0821],\n",
      "        [0.0789],\n",
      "        [0.1134],\n",
      "        [0.0821],\n",
      "        [0.0541],\n",
      "        [0.0875],\n",
      "        [0.1000],\n",
      "        [0.0944],\n",
      "        [0.0526],\n",
      "        [0.0624],\n",
      "        [0.0469],\n",
      "        [0.0655],\n",
      "        [0.0779],\n",
      "        [0.1447],\n",
      "        [0.1611],\n",
      "        [0.0957],\n",
      "        [0.0742],\n",
      "        [0.0953],\n",
      "        [0.0636],\n",
      "        [0.0916],\n",
      "        [0.0877],\n",
      "        [0.0645],\n",
      "        [0.1251],\n",
      "        [0.0913],\n",
      "        [0.1184],\n",
      "        [0.0442],\n",
      "        [0.0604],\n",
      "        [0.1142],\n",
      "        [0.0709],\n",
      "        [0.0650],\n",
      "        [0.1050],\n",
      "        [0.1034],\n",
      "        [0.0876],\n",
      "        [0.1116],\n",
      "        [0.0845],\n",
      "        [0.1141],\n",
      "        [0.0825],\n",
      "        [0.0451],\n",
      "        [0.0731],\n",
      "        [0.0824],\n",
      "        [0.1075],\n",
      "        [0.0673],\n",
      "        [0.0640],\n",
      "        [0.0733],\n",
      "        [0.0624],\n",
      "        [0.0788],\n",
      "        [0.0792],\n",
      "        [0.0487],\n",
      "        [0.0813],\n",
      "        [0.0782],\n",
      "        [0.0854],\n",
      "        [0.0564],\n",
      "        [0.1131],\n",
      "        [0.0667],\n",
      "        [0.0291],\n",
      "        [0.0915],\n",
      "        [0.0849],\n",
      "        [0.0873],\n",
      "        [0.1298],\n",
      "        [0.0877],\n",
      "        [0.1224],\n",
      "        [0.0899],\n",
      "        [0.1222],\n",
      "        [0.0416],\n",
      "        [0.0458],\n",
      "        [0.0635],\n",
      "        [0.0676],\n",
      "        [0.0555],\n",
      "        [0.1363],\n",
      "        [0.0481],\n",
      "        [0.0907],\n",
      "        [0.0625],\n",
      "        [0.0599],\n",
      "        [0.1007],\n",
      "        [0.0518],\n",
      "        [0.0344],\n",
      "        [0.1135],\n",
      "        [0.0705],\n",
      "        [0.1233],\n",
      "        [0.0864],\n",
      "        [0.0777],\n",
      "        [0.0671],\n",
      "        [0.0711],\n",
      "        [0.0734],\n",
      "        [0.1201],\n",
      "        [0.0874],\n",
      "        [0.0892],\n",
      "        [0.0800],\n",
      "        [0.0470]])]\n",
      "[tensor([[0.0805],\n",
      "        [0.1117],\n",
      "        [0.0729],\n",
      "        [0.0965],\n",
      "        [0.0994],\n",
      "        [0.0736],\n",
      "        [0.0606],\n",
      "        [0.0805],\n",
      "        [0.0641],\n",
      "        [0.1030],\n",
      "        [0.0912],\n",
      "        [0.1200],\n",
      "        [0.1023],\n",
      "        [0.0658],\n",
      "        [0.1492],\n",
      "        [0.0933],\n",
      "        [0.0737],\n",
      "        [0.0552],\n",
      "        [0.0743],\n",
      "        [0.0873],\n",
      "        [0.0690],\n",
      "        [0.0696],\n",
      "        [0.1119],\n",
      "        [0.0874],\n",
      "        [0.0699],\n",
      "        [0.1275],\n",
      "        [0.0971],\n",
      "        [0.0446],\n",
      "        [0.0874],\n",
      "        [0.0951],\n",
      "        [0.0452],\n",
      "        [0.1077],\n",
      "        [0.1055],\n",
      "        [0.0667],\n",
      "        [0.0372],\n",
      "        [0.0681],\n",
      "        [0.0453],\n",
      "        [0.0695],\n",
      "        [0.0806],\n",
      "        [0.1096],\n",
      "        [0.0838],\n",
      "        [0.0494],\n",
      "        [0.0809],\n",
      "        [0.0763],\n",
      "        [0.0732],\n",
      "        [0.0986],\n",
      "        [0.1019],\n",
      "        [0.0730],\n",
      "        [0.0907],\n",
      "        [0.0413],\n",
      "        [0.0954],\n",
      "        [0.1107],\n",
      "        [0.1043],\n",
      "        [0.0755],\n",
      "        [0.0455],\n",
      "        [0.0880],\n",
      "        [0.0786],\n",
      "        [0.1291],\n",
      "        [0.0670],\n",
      "        [0.1111],\n",
      "        [0.1082],\n",
      "        [0.1020],\n",
      "        [0.1017],\n",
      "        [0.0612],\n",
      "        [0.0741],\n",
      "        [0.0673],\n",
      "        [0.0757],\n",
      "        [0.0988],\n",
      "        [0.1058],\n",
      "        [0.0645],\n",
      "        [0.0761],\n",
      "        [0.0524],\n",
      "        [0.0533],\n",
      "        [0.0688],\n",
      "        [0.0559],\n",
      "        [0.0966],\n",
      "        [0.1070],\n",
      "        [0.0435],\n",
      "        [0.0635],\n",
      "        [0.0863],\n",
      "        [0.0955],\n",
      "        [0.0919],\n",
      "        [0.1273],\n",
      "        [0.0910],\n",
      "        [0.0901],\n",
      "        [0.0610],\n",
      "        [0.0963],\n",
      "        [0.1229],\n",
      "        [0.0537],\n",
      "        [0.1041],\n",
      "        [0.0742],\n",
      "        [0.0779],\n",
      "        [0.0878],\n",
      "        [0.0625],\n",
      "        [0.0828],\n",
      "        [0.0975],\n",
      "        [0.0644],\n",
      "        [0.0995],\n",
      "        [0.1143],\n",
      "        [0.1060]]), tensor([[0.0955],\n",
      "        [0.0770],\n",
      "        [0.1198],\n",
      "        [0.0626],\n",
      "        [0.0888],\n",
      "        [0.0498],\n",
      "        [0.0644],\n",
      "        [0.0402],\n",
      "        [0.0875],\n",
      "        [0.0895],\n",
      "        [0.0592],\n",
      "        [0.0672],\n",
      "        [0.1026],\n",
      "        [0.0719],\n",
      "        [0.0813],\n",
      "        [0.0909],\n",
      "        [0.0620],\n",
      "        [0.1098],\n",
      "        [0.0367],\n",
      "        [0.1058],\n",
      "        [0.0589],\n",
      "        [0.0638],\n",
      "        [0.0414],\n",
      "        [0.0293],\n",
      "        [0.0701],\n",
      "        [0.0916],\n",
      "        [0.0625],\n",
      "        [0.1152],\n",
      "        [0.0877],\n",
      "        [0.1312],\n",
      "        [0.0639],\n",
      "        [0.1172],\n",
      "        [0.0878],\n",
      "        [0.0856],\n",
      "        [0.0959],\n",
      "        [0.0665],\n",
      "        [0.1148],\n",
      "        [0.0707],\n",
      "        [0.0935],\n",
      "        [0.0727],\n",
      "        [0.0990],\n",
      "        [0.0707],\n",
      "        [0.0987],\n",
      "        [0.0985],\n",
      "        [0.0974],\n",
      "        [0.1137],\n",
      "        [0.0845],\n",
      "        [0.1181],\n",
      "        [0.0576],\n",
      "        [0.0975],\n",
      "        [0.0974],\n",
      "        [0.0987],\n",
      "        [0.1212],\n",
      "        [0.0802],\n",
      "        [0.1199],\n",
      "        [0.0976],\n",
      "        [0.0778],\n",
      "        [0.0740],\n",
      "        [0.0644],\n",
      "        [0.0408],\n",
      "        [0.0956],\n",
      "        [0.1163],\n",
      "        [0.0559],\n",
      "        [0.0664],\n",
      "        [0.0569],\n",
      "        [0.0700],\n",
      "        [0.0731],\n",
      "        [0.0604],\n",
      "        [0.1320],\n",
      "        [0.0824],\n",
      "        [0.1218],\n",
      "        [0.0669],\n",
      "        [0.0744],\n",
      "        [0.0739],\n",
      "        [0.0834],\n",
      "        [0.0689],\n",
      "        [0.0998],\n",
      "        [0.1200],\n",
      "        [0.0784],\n",
      "        [0.0725],\n",
      "        [0.0707],\n",
      "        [0.0861],\n",
      "        [0.0686],\n",
      "        [0.0711],\n",
      "        [0.0982],\n",
      "        [0.0308],\n",
      "        [0.0825],\n",
      "        [0.1310],\n",
      "        [0.0898],\n",
      "        [0.0583],\n",
      "        [0.1266],\n",
      "        [0.0568],\n",
      "        [0.1099],\n",
      "        [0.0897],\n",
      "        [0.0509],\n",
      "        [0.0690],\n",
      "        [0.0454],\n",
      "        [0.0318],\n",
      "        [0.0568],\n",
      "        [0.0766]]), tensor([[0.1258],\n",
      "        [0.0660],\n",
      "        [0.0866],\n",
      "        [0.0450],\n",
      "        [0.0613],\n",
      "        [0.1088],\n",
      "        [0.0832],\n",
      "        [0.0574],\n",
      "        [0.0851],\n",
      "        [0.0942],\n",
      "        [0.1125],\n",
      "        [0.0339],\n",
      "        [0.1188],\n",
      "        [0.1041],\n",
      "        [0.0703],\n",
      "        [0.0852],\n",
      "        [0.0877],\n",
      "        [0.0808],\n",
      "        [0.0889],\n",
      "        [0.0813],\n",
      "        [0.0597],\n",
      "        [0.0579],\n",
      "        [0.1044],\n",
      "        [0.0626],\n",
      "        [0.0560],\n",
      "        [0.0591],\n",
      "        [0.0745],\n",
      "        [0.0954],\n",
      "        [0.0628],\n",
      "        [0.0695],\n",
      "        [0.1196],\n",
      "        [0.0733],\n",
      "        [0.0938],\n",
      "        [0.0777],\n",
      "        [0.0708],\n",
      "        [0.1190],\n",
      "        [0.0622],\n",
      "        [0.0658],\n",
      "        [0.1334],\n",
      "        [0.0668],\n",
      "        [0.0564],\n",
      "        [0.0595],\n",
      "        [0.1020],\n",
      "        [0.1243],\n",
      "        [0.0632],\n",
      "        [0.1039],\n",
      "        [0.0713],\n",
      "        [0.0554],\n",
      "        [0.0813],\n",
      "        [0.1149],\n",
      "        [0.0728],\n",
      "        [0.0778],\n",
      "        [0.0643],\n",
      "        [0.1017],\n",
      "        [0.1247],\n",
      "        [0.0612],\n",
      "        [0.0847],\n",
      "        [0.0607],\n",
      "        [0.0698],\n",
      "        [0.0832],\n",
      "        [0.0672],\n",
      "        [0.0692],\n",
      "        [0.0704],\n",
      "        [0.0580],\n",
      "        [0.0505],\n",
      "        [0.1174],\n",
      "        [0.0832],\n",
      "        [0.0874],\n",
      "        [0.1021],\n",
      "        [0.0692],\n",
      "        [0.0451],\n",
      "        [0.0887],\n",
      "        [0.0706],\n",
      "        [0.0790],\n",
      "        [0.0726],\n",
      "        [0.0584],\n",
      "        [0.0551],\n",
      "        [0.0686],\n",
      "        [0.0750],\n",
      "        [0.0717],\n",
      "        [0.0784],\n",
      "        [0.0868],\n",
      "        [0.0602],\n",
      "        [0.0777],\n",
      "        [0.0613],\n",
      "        [0.0723],\n",
      "        [0.0904],\n",
      "        [0.0868],\n",
      "        [0.0771],\n",
      "        [0.1169],\n",
      "        [0.0503],\n",
      "        [0.0413],\n",
      "        [0.1035],\n",
      "        [0.0710],\n",
      "        [0.1262],\n",
      "        [0.0794],\n",
      "        [0.1070],\n",
      "        [0.0632],\n",
      "        [0.1000],\n",
      "        [0.0706]]), tensor([[0.1152],\n",
      "        [0.0974],\n",
      "        [0.1015],\n",
      "        [0.1059],\n",
      "        [0.1206],\n",
      "        [0.0763],\n",
      "        [0.0342],\n",
      "        [0.0428],\n",
      "        [0.0780],\n",
      "        [0.0643],\n",
      "        [0.0718],\n",
      "        [0.0821],\n",
      "        [0.0789],\n",
      "        [0.1134],\n",
      "        [0.0821],\n",
      "        [0.0541],\n",
      "        [0.0875],\n",
      "        [0.1000],\n",
      "        [0.0944],\n",
      "        [0.0526],\n",
      "        [0.0624],\n",
      "        [0.0469],\n",
      "        [0.0655],\n",
      "        [0.0779],\n",
      "        [0.1447],\n",
      "        [0.1611],\n",
      "        [0.0957],\n",
      "        [0.0742],\n",
      "        [0.0953],\n",
      "        [0.0636],\n",
      "        [0.0916],\n",
      "        [0.0877],\n",
      "        [0.0645],\n",
      "        [0.1251],\n",
      "        [0.0913],\n",
      "        [0.1184],\n",
      "        [0.0442],\n",
      "        [0.0604],\n",
      "        [0.1142],\n",
      "        [0.0709],\n",
      "        [0.0650],\n",
      "        [0.1050],\n",
      "        [0.1034],\n",
      "        [0.0876],\n",
      "        [0.1116],\n",
      "        [0.0845],\n",
      "        [0.1141],\n",
      "        [0.0825],\n",
      "        [0.0451],\n",
      "        [0.0731],\n",
      "        [0.0824],\n",
      "        [0.1075],\n",
      "        [0.0673],\n",
      "        [0.0640],\n",
      "        [0.0733],\n",
      "        [0.0624],\n",
      "        [0.0788],\n",
      "        [0.0792],\n",
      "        [0.0487],\n",
      "        [0.0813],\n",
      "        [0.0782],\n",
      "        [0.0854],\n",
      "        [0.0564],\n",
      "        [0.1131],\n",
      "        [0.0667],\n",
      "        [0.0291],\n",
      "        [0.0915],\n",
      "        [0.0849],\n",
      "        [0.0873],\n",
      "        [0.1298],\n",
      "        [0.0877],\n",
      "        [0.1224],\n",
      "        [0.0899],\n",
      "        [0.1222],\n",
      "        [0.0416],\n",
      "        [0.0458],\n",
      "        [0.0635],\n",
      "        [0.0676],\n",
      "        [0.0555],\n",
      "        [0.1363],\n",
      "        [0.0481],\n",
      "        [0.0907],\n",
      "        [0.0625],\n",
      "        [0.0599],\n",
      "        [0.1007],\n",
      "        [0.0518],\n",
      "        [0.0344],\n",
      "        [0.1135],\n",
      "        [0.0705],\n",
      "        [0.1233],\n",
      "        [0.0864],\n",
      "        [0.0777],\n",
      "        [0.0671],\n",
      "        [0.0711],\n",
      "        [0.0734],\n",
      "        [0.1201],\n",
      "        [0.0874],\n",
      "        [0.0892],\n",
      "        [0.0800],\n",
      "        [0.0470]])]\n",
      "val_auc_acore=0.23730964467005078\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dd28a229e3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m#add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# accuracy and f1 score plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;31m#         f1score[j] = f1_score(targets, predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "import torchvision.models as model\n",
    "model = model.resnet152(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, 100)\n",
    "\n",
    "\n",
    "mymodel = Resnet_152(model)\n",
    "\n",
    "mymodel = mymodel.to(device)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=0.0001,weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, mode='max',verbose=True)\n",
    "\n",
    "#es = EarlyStopping(patience=5, mode=\"max\")\n",
    "\n",
    "for i in range(1):\n",
    "    print('Fold={}'.format(i))\n",
    "    tr_loader, val_loader,targets = train_fold(i+1)\n",
    "    epochs = 1\n",
    "    \n",
    "    # add\n",
    "    acc = {}\n",
    "    f1score= {}\n",
    "    loss_score = {}\n",
    "    time_rec = {}\n",
    "    \n",
    "    \n",
    "    for j in range(epochs):\n",
    "        \n",
    "        loss_arr = []\n",
    "        mymodel.train()\n",
    "        \n",
    "        # add\n",
    "        start_time = time.time()\n",
    "#         start.record()\n",
    "        \n",
    "        for data in tr_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            del data\n",
    "            optimizer.zero_grad()\n",
    "            output = mymodel(images)\n",
    "            #print(output)\n",
    "            loss = criteria(output, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.item())\n",
    "            del images,labels\n",
    "        print(\"epoch={},loss={}\".format(j, sum(loss_arr)/len(loss_arr)))\n",
    "        \n",
    "        \n",
    "        # add\n",
    "#         end.record()\n",
    "#         end.synchronize()\n",
    "        \n",
    "        time_rec[j] = time.time() - start_time #  start.elapsed_time(end)\n",
    "        \n",
    "        loss_score[j] = sum(loss_arr)/len(loss_arr)\n",
    "        \n",
    "        mymodel.eval()\n",
    "        final_predictions = []\n",
    "        \n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            del  val_data\n",
    "            with torch.no_grad():\n",
    "                val_output = mymodel(val_images)\n",
    "                #proba, pred = torch.max(val_output.data, 1)\n",
    "                #print(val_output)\n",
    "                val_output = torch.sigmoid(val_output)\n",
    "                pred = val_output.cpu()\n",
    "                #final_predictions.extend(pred)\n",
    "                final_predictions.append(pred)\n",
    "                del val_images, val_labels\n",
    "#         predictions = np.array(final_predictions)\n",
    "#         print(final_predictions)\n",
    "        predictions = np.vstack(final_predictions).ravel()\n",
    "#         print(final_predictions)\n",
    "        k = roc_auc_score(targets, predictions)\n",
    "        #l=accuracy_score(targets, predictions)\n",
    "        print('val_auc_acore={}'.format(k))\n",
    "        \n",
    "        #add\n",
    "        # accuracy and f1 score plot\n",
    "        acc[j] = k\n",
    "#         f1score[j] = f1_score(targets, predictions)\n",
    "        \n",
    "        scheduler.step(k)\n",
    "#       torch.save(mymodel.state_dict(), 'resnet152_{}.pth'.format(i))  \n",
    "\n",
    "#   add\n",
    "    plt.plot(*zip(*sorted(acc.items())))\n",
    "    plt.show()\n",
    "    plt.savefig('this_is_testing_acc.png')\n",
    "\n",
    "#     plt.plot(*zip(*sorted(f1score.items())))\n",
    "#     plt.show()\n",
    "#     plt.savefig('this_is_testing_2_f1.png')\n",
    "    \n",
    "    plt.plot(*zip(*sorted(loss_score.items())))\n",
    "    plt.show()\n",
    "    plt.savefig('this_is_testing_2_loss.png')\n",
    "    \n",
    "    \n",
    "    plt.plot(*zip(*sorted(time_rec.items())))\n",
    "    plt.show()\n",
    "    plt.savefig('this_is_testing_2_time.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4992.684480667114}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "!\n",
      "2\n",
      "2\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(\"2\")\n",
    "    print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_conversation_512.py  script1.py                       train.csv\r\n",
      "GPU_check.py              SIIM_submission_resnet50 (1).py  train_kfold.csv\r\n",
      "GPU .ipynb                test2.py                         Visualization.ipynb\r\n",
      "Model .ipynb              test3.py\r\n",
      "notebook.ipynb            test.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(start.elapsed_time(end)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANrElEQVR4nO3cf6jdd33H8edrSYtWJxNzh5gfTQbBmRaccsysbiJzQtp16xz9IxWFjbFQsVpFmJ1/OLb/BBG30a0E7fxDMYy2lE7aVWGy+cdWchNbbJqVhfgjt+lo1K21m1Bv994f99SeXE+T721u9k3efT7gwvl+v5/vOe/7JXnme8+9N6kqJEl9/dzYA0iSzi9DL0nNGXpJas7QS1Jzhl6Smts49gDzbNq0qbZv3z72GJJ00Th06ND3q2ph3rELMvTbt29ncXFx7DEk6aKR5LsvdMy3biSpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpuUGhT7InyaNJjiW5Zc7xdyZ5MsmD049PTvdvTfL1JEeTHEly83p/ApKkM9t4tgVJNgC3Au8GloCDSe6pqkdWLf1GVV27at8y8LGqOpzk54FDSb4251xJ0nky5I5+N3Csqo5X1TPAAeC6IU9eVY9X1eHp4x8BR4HNL3ZYSdLaDQn9ZuDEzPYS82N9VZKHktyX5IrVB5NsB94EPDDvRZLsS7KYZPHUqVMDxpIkDTEk9Jmzr1ZtHwYur6o3An8F3H3aEySvBO4EPlJVT817karaX1WTqposLCwMGEuSNMSQ0C8BW2e2twAnZxdU1VNV9fT08b3AJUk2ASS5hJXIf6mq7lqXqSVJgw0J/UFgZ5IdSS4F9gL3zC5I8tokmT7ePX3eH0z3fR44WlWfWd/RJUlDnPWnbqpqOclNwP3ABuD2qjqS5Mbp8duA64EPJFkGfgzsrapK8mvA+4FvJXlw+pSfmN71S5L+H6Rq9dvt45tMJrW4uDj2GJJ00UhyqKom8475m7GS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWpuUOiT7EnyaJJjSW6Zc/ydSZ5M8uD045Mzx25P8kSSh9dzcEnSMGcNfZINwK3A1cAu4IYku+Ys/UZV/cr0489n9n8B2LMew0qS1m7IHf1u4FhVHa+qZ4ADwHVDX6Cq/hn44YucT5J0joaEfjNwYmZ7abpvtauSPJTkviRXrMt0kqRztnHAmszZV6u2DwOXV9XTSa4B7gZ2rmWQJPuAfQDbtm1by6mSpDMYcke/BGyd2d4CnJxdUFVPVdXT08f3Apck2bSWQapqf1VNqmqysLCwllMlSWcwJPQHgZ1JdiS5FNgL3DO7IMlrk2T6ePf0eX+w3sNKktburKGvqmXgJuB+4Cjwd1V1JMmNSW6cLrseeDjJQ8BfAnurqgCSfBn4F+D1SZaS/OH5+EQkSfNl2uMLymQyqcXFxbHHkKSLRpJDVTWZd8zfjJWk5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaGxT6JHuSPJrkWJJb5hx/Z5Inkzw4/fjk0HMlSefXxrMtSLIBuBV4N7AEHExyT1U9smrpN6rq2hd5riTpPDlr6IHdwLGqOg6Q5ABwHTAk1udy7pr92d8f4ZGTT52Pp5ak827X617Fn/72Fev+vEPeutkMnJjZXpruW+2qJA8luS/Jc5MOPZck+5IsJlk8derUgLEkSUMMuaPPnH21avswcHlVPZ3kGuBuYOfAc1d2Vu0H9gNMJpO5a87mfPxLKEkXuyF39EvA1pntLcDJ2QVV9VRVPT19fC9wSZJNQ86VJJ1fQ0J/ENiZZEeSS4G9wD2zC5K8Nkmmj3dPn/cHQ86VJJ1fZ33rpqqWk9wE3A9sAG6vqiNJbpwevw24HvhAkmXgx8Deqipg7rnn6XORJM2RlR5fWCaTSS0uLo49hiRdNJIcqqrJvGP+ZqwkNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4ZekpobFPoke5I8muRYklvOsO4tSZ5Ncv3MvpuTPJzkSJKPrMPMkqQ1OGvok2wAbgWuBnYBNyTZ9QLrPgXcP7PvSuCPgN3AG4Frk+xcn9ElSUMMuaPfDRyrquNV9QxwALhuzroPAXcCT8zsewPwr1X1P1W1DPwT8J5znFmStAZDQr8ZODGzvTTd91NJNrMS8NtWnfsw8I4kr0lyGXANsPXFjytJWquNA9Zkzr5atf1Z4ONV9Wzy/PKqOprkU8DXgKeBh4DluS+S7AP2AWzbtm3AWJKkIYbc0S9x+l34FuDkqjUT4ECS7wDXA3+d5HcBqurzVfXmqnoH8EPg3+e9SFXtr6pJVU0WFhbW9llIkl7QkDv6g8DOJDuAx4C9wHtnF1TVjuceJ/kC8JWqunu6/YtV9USSbcDvAVetz+iSpCHOGvqqWk5yEys/TbMBuL2qjiS5cXp89fvyq92Z5DXAT4APVtV/nuvQkqThhtzRU1X3Aveu2jc38FX1+6u2f/3FDidJOnf+ZqwkNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLU3KDQJ9mT5NEkx5LccoZ1b0nybJLrZ/Z9NMmRJA8n+XKSl63H4JKkYc4a+iQbgFuBq4FdwA1Jdr3Auk8B98/s2wx8GJhU1ZXABmDv+owuSRpiyB39buBYVR2vqmeAA8B1c9Z9CLgTeGLV/o3Ay5NsBC4DTp7DvJKkNRoS+s3AiZntpem+n5reub8HuG12f1U9Bnwa+B7wOPBkVX113osk2ZdkMcniqVOnhn8GkqQzGhL6zNlXq7Y/C3y8qp497cTk1azc/e8AXge8Isn75r1IVe2vqklVTRYWFgaMJUkaYuOANUvA1pntLfzs2y8T4EASgE3ANUmWgUuAb1fVKYAkdwFvA754jnNLkgYaEvqDwM4kO4DHWPlm6ntnF1TVjuceJ/kC8JWqujvJrwJvTXIZ8GPgXcDiOs0uSRrgrKGvquUkN7Hy0zQbgNur6kiSG6fHbzvDuQ8kuQM4DCwD3wT2r8vkkqRBUrX67fbxTSaTWlz0xl+ShkpyqKom8475m7GS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaS1WNPcPPSHIK+O6LPH0T8P11HOdi5rU4ndfjdF6P53W4FpdX1cK8Axdk6M9FksWqmow9x4XAa3E6r8fpvB7P634tfOtGkpoz9JLUXMfQ7x97gAuI1+J0Xo/TeT2e1/patHuPXpJ0uo539JKkGYZekpprE/oke5I8muRYklvGnmdMSbYm+XqSo0mOJLl57JnGlmRDkm8m+crYs4wtyS8kuSPJv03/jFw19kxjSvLR6d+Th5N8OcnLxp5pvbUIfZINwK3A1cAu4IYku8adalTLwMeq6g3AW4EPvsSvB8DNwNGxh7hA/AXwD1X1y8AbeQlflySbgQ8Dk6q6EtgA7B13qvXXIvTAbuBYVR2vqmeAA8B1I880mqp6vKoOTx//iJW/yJvHnWo8SbYAvwV8buxZxpbkVcA7gM8DVNUzVfVfow41vo3Ay5NsBC4DTo48z7rrEvrNwImZ7SVewmGblWQ78CbggZFHGdNngT8G/nfkOS4EvwScAv52+lbW55K8YuyhxlJVjwGfBr4HPA48WVVfHXeq9dcl9Jmz7yX/c6NJXgncCXykqp4ae54xJLkWeKKqDo09ywViI/Bm4G+q6k3AfwMv2e9pJXk1K1/97wBeB7wiyfvGnWr9dQn9ErB1ZnsLDb/8Woskl7AS+S9V1V1jzzOitwO/k+Q7rLyl9xtJvjjuSKNaApaq6rmv8O5gJfwvVb8JfLuqTlXVT4C7gLeNPNO66xL6g8DOJDuSXMrKN1PuGXmm0SQJK+/BHq2qz4w9z5iq6k+qaktVbWflz8U/VlW7O7ahquo/gBNJXj/d9S7gkRFHGtv3gLcmuWz69+ZdNPzm9MaxB1gPVbWc5Cbgfla+a357VR0ZeawxvR14P/CtJA9O932iqu4dbyRdQD4EfGl6U3Qc+IOR5xlNVT2Q5A7gMCs/rfZNGv53CP4XCJLUXJe3biRJL8DQS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpuf8DNjNwrWmJiS8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc = {}\n",
    "f1score= {}\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    y_true = [0, 0, 1, 1, 0]\n",
    "    y_pred = [0, 0, 1, 0, 1]\n",
    "\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "\n",
    "    result = classification_report(y_true, y_pred)\n",
    "    # print( result[\"label 1\"][\"precision\"][2])\n",
    "    acc[i] = accuracy_score(y_true, y_pred) \n",
    "    f1score[i] = f1_score(y_true, y_pred) \n",
    "\n",
    "    \n",
    "plt.plot(*zip(*sorted(acc.items())))\n",
    "plt.show()\n",
    "plt.savefig('this_is_testing.png')\n",
    "\n",
    "plt.plot(*zip(*sorted(f1score.items())))\n",
    "plt.show()\n",
    "plt.savefig('this_is_testing_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import pretrainedmodels\n",
    "\n",
    "class SEResnext50_32x4d(nn.Module):\n",
    "    def __init__(self, pretrained='imagenet'):\n",
    "        super(SEResnext50_32x4d, self).__init__()\n",
    "        \n",
    "        self.base_model = pretrainedmodels.__dict__[\n",
    "            \"se_resnext50_32x4d\"\n",
    "        ](pretrained=pretrained)\n",
    "        self.out = nn.Linear(2048, 1)\n",
    "    \n",
    "    def forward(self, image, targets):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "        \n",
    "        x = self.base_model.features(image)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "        \n",
    "        out = self.l0(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(out, targets.view(-1, 1).type_as(x))\n",
    "\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.56it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_PATHS = ['/scratch/dabas.a/data/jpeg/train_resize']\n",
    "PATH_DICT = {}\n",
    "for folder_path in tqdm(IMG_PATHS):\n",
    "    for img_path in os.listdir(folder_path):\n",
    "        PATH_DICT[img_path] = folder_path + '/'\n",
    "\n",
    "        \n",
    "W = 512\n",
    "H = 512\n",
    "B = 0.5\n",
    "SPLIT = 0.8\n",
    "SAMPLE = True\n",
    "MU = [0.485, 0.456, 0.406]\n",
    "SIGMA = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def to_tensor(data):\n",
    "    return [FloatTensor(point) for point in data]\n",
    "\n",
    "def set_image_transformations(dataset, aug):\n",
    "    norm = Normalize(mean=MU, std=SIGMA, p=1)\n",
    "    vflip, hflip = VerticalFlip(p=0.5), HorizontalFlip(p=0.5)\n",
    "    dataset.transformation = Compose([norm, vflip, hflip]) if aug else norm\n",
    "\n",
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, aug, targ, ids):\n",
    "        set_image_transformations(self, aug)\n",
    "        self.df, self.targ, self.aug, self.image_ids = df, targ, aug, ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image_id = self.image_ids[i]\n",
    "        target = [self.df.target[i]] if self.targ else 0\n",
    "        image = cv2.imread(PATH_DICT[image_id] )\n",
    "        return to_tensor([self.transformation(image=image)['image'], target])\n",
    "    \n",
    "    \n",
    "\n",
    "split = int(SPLIT*len(train_df))\n",
    "train_df, val_df = train_df.loc[:split], train_df.loc[split:]\n",
    "train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "C = np.array([B, (1 - B)])*2\n",
    "ones = len(train_df.query('target == 1'))\n",
    "zeros = len(train_df.query('target == 0'))\n",
    "\n",
    "weightage_fn = {0: C[1]/zeros, 1: C[0]/ones}\n",
    "weights = [weightage_fn[target] for target in train_df.target]\n",
    "\n",
    "length = len(train_df)\n",
    "val_ids = val_df.image_name.apply(lambda x: x + '.jpg')\n",
    "train_ids = train_df.image_name.apply(lambda x: x + '.jpg')\n",
    "\n",
    "val_set = SIIMDataset(val_df, False, True, val_ids)\n",
    "train_set = SIIMDataset(train_df, True, True, train_ids)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "train_sampler = WeightedRandomSampler(weights, length)\n",
    "if_sample, if_shuffle = (train_sampler, False), (None, True)\n",
    "sample_fn = lambda is_sample, sampler: if_sample if is_sample else if_shuffle\n",
    "\n",
    "sampler, shuffler = sample_fn(SAMPLE, train_sampler)\n",
    "val_loader = DataLoader(val_set, VAL_BATCH_SIZE, shuffle=False)\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, sampler=sampler, shuffle=shuffler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc02abe93d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6b6342a5a966>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarg\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"force_apply must have bool or int type\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mneed_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_apply\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m_check_args\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minternal_data_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecked_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} must be numpy array type\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minternal_data_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecked_multi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: image must be numpy array type"
     ]
    }
   ],
   "source": [
    "for images, _ in train_loader:\n",
    "    print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save graphs \n",
    "\n",
    "plt.savefig('books_read.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
